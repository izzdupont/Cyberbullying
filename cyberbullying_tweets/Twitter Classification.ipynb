{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee1226f7",
   "metadata": {},
   "source": [
    "# Library and data importing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7ac375e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c38ffce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "religion               7998\n",
       "age                    7992\n",
       "gender                 7973\n",
       "ethnicity              7961\n",
       "not_cyberbullying      7945\n",
       "other_cyberbullying    7823\n",
       "Name: cyberbullying_type, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('cyberbullying_tweets.csv')\n",
    "x = data.iloc[:,0:1].values\n",
    "y = data.iloc[:,-1].values\n",
    "data.cyberbullying_type.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f439110",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        In other words #katandandre, your food was cra...\n",
       "1        Why is #aussietv so white? #MKR #theblock #ImA...\n",
       "2        @XochitlSuckkks a classy whore? Or more red ve...\n",
       "3        @Jason_Gio meh. :P  thanks for the heads up, b...\n",
       "4        @RudhoeEnglish This is an ISIS account pretend...\n",
       "                               ...                        \n",
       "47687    Black ppl aren't expected to do anything, depe...\n",
       "47688    Turner did not withhold his disappointment. Tu...\n",
       "47689    I swear to God. This dumb nigger bitch. I have...\n",
       "47690    Yea fuck you RT @therealexel: IF YOURE A NIGGE...\n",
       "47691    Bro. U gotta chill RT @CHILLShrammy: Dog FUCK ...\n",
       "Name: tweet_text, Length: 47692, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['tweet_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee81913d",
   "metadata": {},
   "source": [
    "\n",
    "# Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "184a3cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.tweet_text=[doc.lower() for doc in data.tweet_text] \n",
    "#to lower string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3369719",
   "metadata": {},
   "outputs": [],
   "source": [
    "string.punctuation\n",
    "def remove_Punc(teststring):\n",
    "    newstring = \"\".join([i for i in teststring if i not in string.punctuation])\n",
    "    return newstring\n",
    "data['tweet_text_1']=data['tweet_text'].apply(lambda x : remove_Punc(x))\n",
    "data['tweet_text']=data['tweet_text_1']\n",
    "data['tweet_text'] = [x.replace(\"\\n\\r\",\"\") for x in data['tweet_text']]\n",
    "data.drop('tweet_text_1',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "00ba9cd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>cyberbullying_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>47672</th>\n",
       "      <td>a harvard jd doesnt imply dumb rt staxxfifth a...</td>\n",
       "      <td>ethnicity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47673</th>\n",
       "      <td>â€œranran42 everyone stays hatin sorry i like to...</td>\n",
       "      <td>ethnicity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47674</th>\n",
       "      <td>fuck you doris ya dumb nigger</td>\n",
       "      <td>ethnicity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47675</th>\n",
       "      <td>yall make sure yall following a nigga coonâ›½ðŸ…°ðŸ†–ðŸ’¯...</td>\n",
       "      <td>ethnicity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47676</th>\n",
       "      <td>dumb fuck wipe your ass bitch nigger cock munc...</td>\n",
       "      <td>ethnicity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47677</th>\n",
       "      <td>why the fuck am i a dumb fucking nigger you du...</td>\n",
       "      <td>ethnicity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47678</th>\n",
       "      <td>why have i got someone on my timeline sayin fu...</td>\n",
       "      <td>ethnicity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47679</th>\n",
       "      <td>iâ€™ve noticed itâ€™s a trend on here yâ€™all have t...</td>\n",
       "      <td>ethnicity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47680</th>\n",
       "      <td>so a random iowa fan pulls up in his car and s...</td>\n",
       "      <td>ethnicity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47681</th>\n",
       "      <td>as a black man iâ€™m not supporting music that t...</td>\n",
       "      <td>ethnicity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47682</th>\n",
       "      <td>but heâ€™s right atlanta is full of so called sh...</td>\n",
       "      <td>ethnicity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47683</th>\n",
       "      <td>black is a color  african american is a cultur...</td>\n",
       "      <td>ethnicity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47684</th>\n",
       "      <td>after black soldiers fought in wwi many of the...</td>\n",
       "      <td>ethnicity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47685</th>\n",
       "      <td>keithbishop64 very true it was a nice show to ...</td>\n",
       "      <td>ethnicity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47686</th>\n",
       "      <td>yourfavwhiteguy shut the fuck upabout your dum...</td>\n",
       "      <td>ethnicity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47687</th>\n",
       "      <td>black ppl arent expected to do anything depend...</td>\n",
       "      <td>ethnicity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47688</th>\n",
       "      <td>turner did not withhold his disappointment tur...</td>\n",
       "      <td>ethnicity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47689</th>\n",
       "      <td>i swear to god this dumb nigger bitch i have g...</td>\n",
       "      <td>ethnicity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47690</th>\n",
       "      <td>yea fuck you rt therealexel if youre a nigger ...</td>\n",
       "      <td>ethnicity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47691</th>\n",
       "      <td>bro u gotta chill rt chillshrammy dog fuck kp ...</td>\n",
       "      <td>ethnicity</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              tweet_text cyberbullying_type\n",
       "47672  a harvard jd doesnt imply dumb rt staxxfifth a...          ethnicity\n",
       "47673  â€œranran42 everyone stays hatin sorry i like to...          ethnicity\n",
       "47674                      fuck you doris ya dumb nigger          ethnicity\n",
       "47675  yall make sure yall following a nigga coonâ›½ðŸ…°ðŸ†–ðŸ’¯...          ethnicity\n",
       "47676  dumb fuck wipe your ass bitch nigger cock munc...          ethnicity\n",
       "47677  why the fuck am i a dumb fucking nigger you du...          ethnicity\n",
       "47678  why have i got someone on my timeline sayin fu...          ethnicity\n",
       "47679  iâ€™ve noticed itâ€™s a trend on here yâ€™all have t...          ethnicity\n",
       "47680  so a random iowa fan pulls up in his car and s...          ethnicity\n",
       "47681  as a black man iâ€™m not supporting music that t...          ethnicity\n",
       "47682  but heâ€™s right atlanta is full of so called sh...          ethnicity\n",
       "47683  black is a color  african american is a cultur...          ethnicity\n",
       "47684  after black soldiers fought in wwi many of the...          ethnicity\n",
       "47685  keithbishop64 very true it was a nice show to ...          ethnicity\n",
       "47686  yourfavwhiteguy shut the fuck upabout your dum...          ethnicity\n",
       "47687  black ppl arent expected to do anything depend...          ethnicity\n",
       "47688  turner did not withhold his disappointment tur...          ethnicity\n",
       "47689  i swear to god this dumb nigger bitch i have g...          ethnicity\n",
       "47690  yea fuck you rt therealexel if youre a nigger ...          ethnicity\n",
       "47691  bro u gotta chill rt chillshrammy dog fuck kp ...          ethnicity"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5f9e4af2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ed15dbe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "import emoji\n",
    "\n",
    "def remove(string):\n",
    "    string=re.sub(r'https?://\\S+|www\\.\\S+','',string)\n",
    "    string=re.sub(r'@[A-Za-z0-9]+','',string)\n",
    "    string=re.sub(emoji.get_emoji_regexp(),'',string)\n",
    "    string=re.sub(r'#','',string)\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "14d8a74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['tweet_text']=data['tweet_text'].apply(lambda x: remove(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ac33d300",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove extra stuff\n",
    "\n",
    "\n",
    "lemma = WordNetLemmatizer()\n",
    "stopword = stopwords.words('english')\n",
    "stopword = stopword.extend(['etc','didnt','cant','yall','ll','ill','couldnt','ur','dont','^a-z$','^A-Z$'])\n",
    "def lemma_word(string):\n",
    "    for i in str(string).split():\n",
    "        return ''.join(lemma.lemmatize(i))\n",
    "def stop(string):\n",
    "    for i in str(string).split():\n",
    "        if i not in stopword:\n",
    "            return ''.join(i)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cb7caf5d",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "argument of type 'NoneType' is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_20920/21091049.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'tweet_text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'tweet_text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mlemma_word\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'tweet_text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'tweet_text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mstop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[0;32m   4355\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfloat64\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4356\u001b[0m         \"\"\"\n\u001b[1;32m-> 4357\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mSeriesApply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert_dtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4358\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4359\u001b[0m     def _reduce(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\apply.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1041\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1042\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1043\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1044\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1045\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0magg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1096\u001b[0m                 \u001b[1;31m# List[Union[Callable[..., Any], str]]]]]\"; expected\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1097\u001b[0m                 \u001b[1;31m# \"Callable[[Any], Any]\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1098\u001b[1;33m                 mapped = lib.map_infer(\n\u001b[0m\u001b[0;32m   1099\u001b[0m                     \u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m                     \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# type: ignore[arg-type]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_20920/21091049.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'tweet_text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'tweet_text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mlemma_word\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'tweet_text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'tweet_text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mstop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_20920/2924290774.py\u001b[0m in \u001b[0;36mstop\u001b[1;34m(string)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mstop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstopword\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: argument of type 'NoneType' is not iterable"
     ]
    }
   ],
   "source": [
    "data['tweet_text']=data['tweet_text'].apply(lambda x : lemma_word(x))\n",
    "data['tweet_text']=data['tweet_text'].apply(lambda x : stop(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7a9c80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc11ab1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698cc535",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
